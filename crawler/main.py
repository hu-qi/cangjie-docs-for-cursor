#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»“é¢‰æ–‡æ¡£çˆ¬è™«ä¸»ç¨‹åº
"""

import json
import os
import sys
import argparse
import asyncio
from urllib.parse import urljoin
import requests
from playwright.async_api import async_playwright

from html_parser import HTMLParser
from markdown_converter import MarkdownConverter
from image_downloader import ImageDownloader


class CangjieCrawler:
    def __init__(self, version="1.0.1", output_dir="../docs"):
        self.version = version
        self.base_url = f"https://docs.cangjie-lang.cn/docs/{version}/"
        self.search_index_url = f"https://docs.cangjie-lang.cn/docs/{version}/searchindex.json"
        self.output_dir = os.path.abspath(output_dir)
        
        self.html_parser = HTMLParser()
        self.markdown_converter = MarkdownConverter(self.output_dir)
        self.image_downloader = ImageDownloader(self.base_url, self.output_dir, self.version)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.version_output_dir = os.path.join(self.output_dir, version)
        os.makedirs(self.version_output_dir, exist_ok=True)
        
    async def get_page_urls(self):
        """ä»æœç´¢ç´¢å¼•è·å–æ‰€æœ‰é¡µé¢URL"""
        try:
            print(f"è·å–æœç´¢ç´¢å¼•: {self.search_index_url}")
            response = requests.get(self.search_index_url, timeout=30)
            response.raise_for_status()
            
            search_data = response.json()
            doc_urls = search_data.get('doc_urls', [])
            
            # æå–å”¯ä¸€çš„é¡µé¢URLï¼ˆç§»é™¤é”šç‚¹ï¼‰
            page_urls = set()
            for doc_url in doc_urls:
                if '#' in doc_url:
                    page_url = doc_url.split('#')[0]
                else:
                    page_url = doc_url
                    
                if page_url and page_url.endswith('.html'):
                    full_url = urljoin(self.base_url, page_url)
                    page_urls.add((page_url, full_url))
            
            print(f"å‘ç° {len(page_urls)} ä¸ªé¡µé¢")
            return list(page_urls)
            
        except Exception as e:
            print(f"è·å–é¡µé¢URLå¤±è´¥: {e}")
            return []
    
    async def crawl_page(self, page, page_info):
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        relative_url, full_url = page_info
        
        try:
            print(f"æ­£åœ¨çˆ¬å–: {relative_url}")
            
            # è®¿é—®é¡µé¢
            response = await page.goto(full_url, wait_until="networkidle")
            
            if response.status != 200:
                print(f"é¡µé¢è®¿é—®å¤±è´¥: {full_url} (çŠ¶æ€ç : {response.status})")
                return None
                
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            await page.wait_for_load_state("networkidle")
            
            # è·å–é¡µé¢HTML
            html_content = await page.content()
            
            # è§£æHTML
            parsed_content = self.html_parser.parse_page(html_content, full_url)
            
            if not parsed_content:
                print(f"è§£æé¡µé¢å¤±è´¥: {relative_url}")
                return None
            
            # è½¬æ¢ä¸ºMarkdown
            markdown_file_path = relative_url.replace('.html', '.md')
            markdown_content = self.markdown_converter.convert_to_markdown(
                parsed_content, markdown_file_path, self.version
            )
            
            # è¿”å›é¡µé¢æ•°æ®ï¼ŒåŒ…å«åŸå§‹HTMLç”¨äºåç»­å›¾ç‰‡ä¸‹è½½
            return {
                'url': relative_url,
                'title': parsed_content['title'],
                'file_path': markdown_file_path,
                'markdown_content': markdown_content,
                'html_content': html_content,
                'page_url': full_url
            }
            
        except Exception as e:
            print(f"çˆ¬å–é¡µé¢å¤±è´¥ {relative_url}: {e}")
            return None
    
    async def crawl_all_pages(self):
        """çˆ¬å–æ‰€æœ‰é¡µé¢"""
        # è·å–é¡µé¢URLåˆ—è¡¨
        page_urls = await self.get_page_urls()
        
        if not page_urls:
            print("æ²¡æœ‰æ‰¾åˆ°é¡µé¢URL")
            return []
        
        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            )
            
            page = await context.new_page()
            
            # çˆ¬å–æ‰€æœ‰é¡µé¢
            results = []
            for i, page_info in enumerate(page_urls):
                print(f"è¿›åº¦: {i+1}/{len(page_urls)}")
                result = await self.crawl_page(page, page_info)
                if result:
                    results.append(result)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
                await asyncio.sleep(1)
            
            await browser.close()
        
        # ä¸‹è½½å›¾ç‰‡
        if results:
            print("\nğŸ–¼ï¸  å¼€å§‹ä¸‹è½½é¡µé¢å›¾ç‰‡...")
            
            # å‡†å¤‡å›¾ç‰‡ä¸‹è½½æ•°æ®
            image_download_data = []
            for result in results:
                image_download_data.append({
                    'html_content': result['html_content'],
                    'page_url': result['page_url'],
                    'markdown_file_path': result['file_path']
                })
            
            # ä¸‹è½½æ‰€æœ‰å›¾ç‰‡
            image_mappings = await self.image_downloader.download_all_images(image_download_data)
            
            # æ›´æ–°Markdownæ–‡ä»¶ä¸­çš„å›¾ç‰‡è·¯å¾„
            for result in results:
                file_path = result['file_path']
                if file_path in image_mappings:
                    print(f"ğŸ”„ æ›´æ–°å›¾ç‰‡è·¯å¾„: {file_path}")
                    updated_content = self.image_downloader.update_markdown_image_paths(
                        result['markdown_content'], 
                        image_mappings[file_path]
                    )
                    result['markdown_content'] = updated_content
                
                # ä¿å­˜æœ€ç»ˆçš„Markdownæ–‡ä»¶
                full_markdown_path = os.path.join(self.version, file_path)
                self.markdown_converter.save_markdown(result['markdown_content'], full_markdown_path)
            
        return results
    
    def generate_sidebar_config(self, results):
        """æ ¹æ®çˆ¬å–ç»“æœç”Ÿæˆä¾§è¾¹æ é…ç½®"""
        sidebar_config = {}
        
        # æŒ‰ç›®å½•ç»“æ„ç»„ç»‡æ–‡æ¡£
        for result in results:
            file_path = result['file_path']
            title = result['title']
            
            # è§£æç›®å½•ç»“æ„
            path_parts = file_path.split('/')
            
            if len(path_parts) == 1:
                # æ ¹ç›®å½•æ–‡ä»¶
                if 'root' not in sidebar_config:
                    sidebar_config['root'] = []
                sidebar_config['root'].append({
                    'text': title,
                    'link': f'/{self.version}/{file_path}'
                })
            else:
                # å­ç›®å½•æ–‡ä»¶
                category = path_parts[0]
                if category not in sidebar_config:
                    sidebar_config[category] = []
                
                sidebar_config[category].append({
                    'text': title,
                    'link': f'/{self.version}/{file_path}'
                })
        
        return sidebar_config
    
    def save_manifest(self, results):
        """ä¿å­˜çˆ¬å–ç»“æœæ¸…å•"""
        manifest = {
            'version': self.version,
            'total_pages': len(results),
            'pages': results,
            'generated_at': self._get_current_time()
        }
        
        manifest_path = os.path.join(self.version_output_dir, 'manifest.json')
        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)
        
        print(f"ä¿å­˜æ¸…å•æ–‡ä»¶: {manifest_path}")
    
    def _get_current_time(self):
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')


async def main():
    parser = argparse.ArgumentParser(description='ä»“é¢‰æ–‡æ¡£çˆ¬è™«')
    parser.add_argument('--version', '-v', default='1.0.1', help='ä»“é¢‰ç‰ˆæœ¬å·')
    parser.add_argument('--output', '-o', default='../docs', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    print(f"å¼€å§‹çˆ¬å–ä»“é¢‰ {args.version} ç‰ˆæœ¬æ–‡æ¡£...")
    
    crawler = CangjieCrawler(version=args.version, output_dir=args.output)
    results = await crawler.crawl_all_pages()
    
    if results:
        print(f"\nçˆ¬å–å®Œæˆï¼æˆåŠŸå¤„ç† {len(results)} ä¸ªé¡µé¢")
        
        # ä¿å­˜æ¸…å•
        crawler.save_manifest(results)
        
        # ç”Ÿæˆä¾§è¾¹æ é…ç½®
        sidebar_config = crawler.generate_sidebar_config(results)
        print("\nå»ºè®®çš„ä¾§è¾¹æ é…ç½®:")
        print(json.dumps(sidebar_config, ensure_ascii=False, indent=2))
    else:
        print("çˆ¬å–å¤±è´¥æˆ–æ²¡æœ‰æ‰¾åˆ°é¡µé¢")


if __name__ == "__main__":
    asyncio.run(main())