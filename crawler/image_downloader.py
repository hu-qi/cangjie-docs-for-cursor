#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾ç‰‡ä¸‹è½½å™¨ - ä¸‹è½½æ–‡æ¡£ä¸­çš„å›¾ç‰‡èµ„æº
"""

import os
import asyncio
import aiohttp
import aiofiles
from pathlib import Path
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Set


class ImageDownloader:
    def __init__(self, base_url: str, output_dir: str, version: str):
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.version = version
        self.version_dir = self.output_dir / version
        self.downloaded_images: Set[str] = set()
        self.failed_images: Set[str] = set()
        
    async def download_image(self, session: aiohttp.ClientSession, img_url: str, local_path: Path) -> bool:
        """ä¸‹è½½å•ä¸ªå›¾ç‰‡"""
        try:
            # å¦‚æœå·²ç»ä¸‹è½½è¿‡ï¼Œè·³è¿‡
            if str(local_path) in self.downloaded_images:
                return True
                
            # åˆ›å»ºç›®å½•
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            print(f"ğŸ“¥ ä¸‹è½½å›¾ç‰‡: {img_url}")
            
            async with session.get(img_url) as response:
                if response.status == 200:
                    content = await response.read()
                    async with aiofiles.open(local_path, 'wb') as f:
                        await f.write(content)
                    
                    self.downloaded_images.add(str(local_path))
                    print(f"âœ… ä¸‹è½½æˆåŠŸ: {local_path.relative_to(self.version_dir)}")
                    return True
                else:
                    print(f"âŒ ä¸‹è½½å¤±è´¥ ({response.status}): {img_url}")
                    self.failed_images.add(img_url)
                    return False
                    
        except Exception as e:
            print(f"âŒ ä¸‹è½½é”™è¯¯: {img_url} - {e}")
            self.failed_images.add(img_url)
            return False
    
    def extract_images_from_html(self, html_content: str, page_url: str) -> List[Dict[str, str]]:
        """ä»HTMLå†…å®¹ä¸­æå–å›¾ç‰‡ä¿¡æ¯"""
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(html_content, 'html.parser')
        images = []
        
        for img in soup.find_all('img'):
            src = img.get('src')
            alt = img.get('alt', '')
            
            if src:
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if src.startswith('./') or src.startswith('../'):
                    full_url = urljoin(page_url, src)
                elif src.startswith('/'):
                    # ç›¸å¯¹äºåŸŸåæ ¹è·¯å¾„
                    base_domain = '/'.join(page_url.split('/')[:3])
                    full_url = base_domain + src
                elif src.startswith('http'):
                    # ç»å¯¹è·¯å¾„
                    full_url = src
                else:
                    # ç›¸å¯¹äºå½“å‰é¡µé¢
                    full_url = urljoin(page_url, src)
                
                images.append({
                    'src': src,
                    'full_url': full_url,
                    'alt': alt,
                    'page_url': page_url
                })
        
        return images
    
    def get_local_image_path(self, img_info: Dict[str, str], markdown_file_path: str) -> Path:
        """è®¡ç®—å›¾ç‰‡çš„æœ¬åœ°å­˜å‚¨è·¯å¾„"""
        # è·å–å›¾ç‰‡æ–‡ä»¶å
        parsed_url = urlparse(img_info['full_url'])
        img_filename = os.path.basename(parsed_url.path)
        
        if not img_filename or '.' not in img_filename:
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶åæˆ–æ‰©å±•åï¼Œå°è¯•ä»URLå‚æ•°æˆ–ä½¿ç”¨é»˜è®¤
            img_filename = f"image_{hash(img_info['full_url']) % 10000}.png"
        
        # è®¡ç®—ç›¸å¯¹äºmarkdownæ–‡ä»¶çš„imagesç›®å½•
        md_dir = Path(markdown_file_path).parent
        images_dir = self.version_dir / md_dir / 'images'
        
        return images_dir / img_filename
    
    async def download_images_for_page(self, session: aiohttp.ClientSession, 
                                     html_content: str, page_url: str, 
                                     markdown_file_path: str) -> Dict[str, str]:
        """ä¸ºå•ä¸ªé¡µé¢ä¸‹è½½æ‰€æœ‰å›¾ç‰‡ï¼Œè¿”å›åŸå§‹URLåˆ°æœ¬åœ°è·¯å¾„çš„æ˜ å°„"""
        images = self.extract_images_from_html(html_content, page_url)
        url_mapping = {}
        
        if not images:
            return url_mapping
        
        print(f"ğŸ–¼ï¸  é¡µé¢ {markdown_file_path} åŒ…å« {len(images)} ä¸ªå›¾ç‰‡")
        
        for img_info in images:
            local_path = self.get_local_image_path(img_info, markdown_file_path)
            
            # ä¸‹è½½å›¾ç‰‡
            success = await self.download_image(session, img_info['full_url'], local_path)
            
            if success:
                # è®¡ç®—ç›¸å¯¹äºmarkdownæ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
                md_path = self.version_dir / markdown_file_path
                try:
                    relative_path = os.path.relpath(local_path, md_path.parent)
                    url_mapping[img_info['src']] = relative_path
                except ValueError:
                    # å¦‚æœæ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
                    url_mapping[img_info['src']] = str(local_path)
            else:
                # ä¸‹è½½å¤±è´¥ï¼Œä¿æŒåŸå§‹è·¯å¾„æˆ–ç”Ÿæˆå ä½ç¬¦
                url_mapping[img_info['src']] = f"ğŸ“· **[å›¾ç‰‡ç¼ºå¤±]** {img_info['alt']}"
        
        return url_mapping
    
    async def download_all_images(self, page_data: List[Dict]) -> Dict[str, Dict[str, str]]:
        """ä¸‹è½½æ‰€æœ‰é¡µé¢çš„å›¾ç‰‡
        
        Args:
            page_data: åŒ…å«é¡µé¢ä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
                - html_content: HTMLå†…å®¹
                - page_url: é¡µé¢URL
                - markdown_file_path: Markdownæ–‡ä»¶è·¯å¾„
        
        Returns:
            å­—å…¸ï¼Œé”®ä¸ºmarkdownæ–‡ä»¶è·¯å¾„ï¼Œå€¼ä¸ºè¯¥é¡µé¢çš„URLæ˜ å°„å­—å…¸
        """
        print("ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½å›¾ç‰‡...")
        
        all_mappings = {}
        
        # åˆ›å»ºHTTPä¼šè¯
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(
            timeout=timeout,
            headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'}
        ) as session:
            
            for i, data in enumerate(page_data):
                print(f"ğŸ“„ å¤„ç†é¡µé¢ {i+1}/{len(page_data)}: {data['markdown_file_path']}")
                
                mapping = await self.download_images_for_page(
                    session,
                    data['html_content'],
                    data['page_url'],
                    data['markdown_file_path']
                )
                
                if mapping:
                    all_mappings[data['markdown_file_path']] = mapping
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                await asyncio.sleep(0.5)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        total_downloaded = len(self.downloaded_images)
        total_failed = len(self.failed_images)
        
        print(f"\nğŸ‰ å›¾ç‰‡ä¸‹è½½å®Œæˆ!")
        print(f"âœ… æˆåŠŸä¸‹è½½: {total_downloaded} ä¸ªå›¾ç‰‡")
        print(f"âŒ ä¸‹è½½å¤±è´¥: {total_failed} ä¸ªå›¾ç‰‡")
        
        if self.failed_images:
            print("\nâŒ å¤±è´¥çš„å›¾ç‰‡URL:")
            for failed_url in sorted(self.failed_images):
                print(f"   - {failed_url}")
        
        return all_mappings
    
    def update_markdown_image_paths(self, markdown_content: str, url_mapping: Dict[str, str]) -> str:
        """æ›´æ–°Markdownå†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„"""
        import re
        
        def replace_image(match):
            alt_text = match.group(1)
            img_src = match.group(2)
            
            # æŸ¥æ‰¾æ˜¯å¦æœ‰æ–°çš„è·¯å¾„æ˜ å°„
            if img_src in url_mapping:
                new_src = url_mapping[img_src]
                return f"![{alt_text}]({new_src})"
            
            return match.group(0)  # ä¿æŒåŸæ ·
        
        # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡å¼•ç”¨
        img_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        updated_content = re.sub(img_pattern, replace_image, markdown_content)
        
        return updated_content